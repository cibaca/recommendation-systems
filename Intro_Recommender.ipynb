{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04EroEmagG0S","executionInfo":{"status":"ok","timestamp":1651239232894,"user_tz":240,"elapsed":15348,"user":{"displayName":"Cibaca Pramod Khandelwal","userId":"16594370059923439368"}},"outputId":"aa0d7257-b6f5-4491-e8cd-5bc8a3d42d67"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["#Set your project path \n","project_path =  '/content/gdrive/MyDrive/dl-rl/recommendation-systems/'"],"metadata":{"id":"fkIh1rdggL1b","executionInfo":{"status":"ok","timestamp":1651239234257,"user_tz":240,"elapsed":7,"user":{"displayName":"Cibaca Pramod Khandelwal","userId":"16594370059923439368"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2iLd0XuWgLzI","executionInfo":{"status":"ok","timestamp":1651238848880,"user_tz":240,"elapsed":6,"user":{"displayName":"Cibaca Pramod Khandelwal","userId":"16594370059923439368"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKJn7k6Pfkgn"},"source":["# Introduction to Recommendation Systems\n","\n","In this tutorial we used a [deep autoencoder](https://arxiv.org/abs/1708.01715) to create a recommender system with the [Netflix dataset](https://netflixprize.com/). \n","\n","The deep autoencoder in this tutorial is done with [PyTorch](http://pytorch.org/) and is based on [this repo](https://github.com/NVIDIA/DeepRecommender) by NVIDIA. \n","\n","## Overview of Recommendation Systems\n","\n","A [recommendation system](https://en.wikipedia.org/wiki/Recommender_system) seeks to understand the user preferences with the objective of recommending him or her items. These systems has become increasingly popular in recent years, in parallel with the growth of internet retailers like Amazon, Netflix or Spotify. Recommender systems are used in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. In terms of business impact, according to a recent [study](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.895.3477&rep=rep1&type=pdf) from Wharton School, recommendation engines can cause a 25% lift in number of views and 35% lift in number of items purchased. So it is worth to understand these systems.   \n","\n","\n","Generally speaking, there are 3 methodologies for recommendation systems: collaborative filtering, content-based filtering and hybrid recommender systems.\n","\n","[**Collaborative filtering**](https://en.wikipedia.org/wiki/Collaborative_filtering) collects large amounts of information on users’ behaviors, activities or preferences in order to predict what users will like based on their similarity to other users. This information can be explicit, where the user provides directly the ratings of the items, or implicit, where the ratings have to be extracted for the implicit user behavior, like number of views, likes, purchases, etc. \n","\n","<p align=\"center\">\n","    <img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif\" width=300px/>\n","</p>\n","\n","Mathematically, it is based on inferring the missing entries in an `mxn` matrix, `R`, whose `(i, j)` entry describes the ratings given by the `ith` user to the `jth` item. The performance is then measured using Root Mean Squared Error (RMSE). This problem has been addressed in a variaety of ways. Traditional methods include low rank matrix factorization like [Alternating Least Squares](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf), which became popular due to its implementation in [Spark](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html). Methods based on Deep Learning have grown in popularity recently, some techniques use embbedings and RELU activations like in [Youtube](http://dl.acm.org/citation.cfm?doid=2959100.2959190), recurrent neural networks like in [Hedasi et al., 2016](http://arxiv.org/abs/1511.06939) or deep autoencoders like the present tutorial.\n","\n","**Content-based filtering** take into account contextual user factors such as location, date of purchase, user demographics and item factors like price, brand, type of item, etc, to recommend items that are similar to those that a user liked in the past.\n","\n","The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers, cluster analysis, decision trees, and neural networks in order to estimate the probability that the user is going to like the item.\n","\n","**Hybrid recommender systems** combine multiple techniques together to achieve some synergy between them. They can use aspects from collaborative filtering, content-based filtering, knowledge based and demographics. They have proved to be very effective in some cases, like the [Bellkor solution](https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf), winner of the Netflix prize, the [Netflix Recommender System](http://delivery.acm.org/10.1145/2850000/2843948/a13-gomez-uribe.pdf) or [LightFM](https://github.com/lyst/lightfm). \n","\n","In this tutorial we focused on collaborative filtering with autoencoders. We used data of the Netflix prize "]},{"cell_type":"code","source":["!pip install aiohttp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQKuMDJmgV9Z","executionInfo":{"status":"ok","timestamp":1651239246016,"user_tz":240,"elapsed":6581,"user":{"displayName":"Cibaca Pramod Khandelwal","userId":"16594370059923439368"}},"outputId":"6bc044b1-9afd-4591-9336-0f0900186b51"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 38.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 81 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 163 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 296 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 327 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 378 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 409 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 460 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 512 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 542 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 573 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 593 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 624 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 655 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 675 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 706 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 727 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 757 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 788 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 819 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 839 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 870 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 890 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 901 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 921 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 952 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 972 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.0 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 27.6 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (2.0.12)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 54.7 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 74.5 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (4.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (21.4.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp) (2.10)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, aiohttp\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 multidict-6.0.2 yarl-1.7.2\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEP8zuy_fkgq","executionInfo":{"status":"ok","timestamp":1651239250135,"user_tz":240,"elapsed":4124,"user":{"displayName":"Cibaca Pramod Khandelwal","userId":"16594370059923439368"}},"outputId":"6de2252e-2092-4dbb-c689-460d7d373e9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["OS:  linux\n","Python:  3.7.13 (default, Apr 24 2022, 01:04:09) \n","[GCC 7.5.0]\n","PyTorch:  1.11.0+cu113\n","Numpy:  1.21.6\n","Number of CPU processors:  2\n","GPU:  ['Tesla T4']\n","GPU memory:  ['15109 MiB']\n","CUDA:  No CUDA in this machine\n"]}],"source":["import sys\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import aiohttp\n","import asyncio\n","import json\n","import requests\n","\n","sys.path.append('/content/gdrive/MyDrive/dl-rl/recommendation-systems/')\n","from utils import get_gpu_name, get_number_processors, get_gpu_memory, get_cuda_version\n","from parameters import *\n","\n","\n","print(\"OS: \", sys.platform)\n","print(\"Python: \", sys.version)\n","print(\"PyTorch: \", torch.__version__)\n","print(\"Numpy: \", np.__version__)\n","print(\"Number of CPU processors: \", get_number_processors())\n","print(\"GPU: \", get_gpu_name())\n","print(\"GPU memory: \", get_gpu_memory())\n","print(\"CUDA: \", get_cuda_version())\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"WrYKJ_u9fkgr"},"source":["## Dataset: Netflix\n","\n","This dataset was constructed to support participants in the [Netflix Prize](http://www.netflixprize.com). The movie rating files contain over 100 million ratings from 480 thousand randomly-chosen, anonymous Netflix customers over 17 thousand movie titles.  The data were collected between October, 1998 and December, 2005 and reflect the distribution of all ratings received during this period.  The ratings are on a scale from 1 to 5 (integral) stars.\n","\n","The dataset can be [downloaded here](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a). To uncompress it:\n","\n","```bash\n","tar -xvf nf_prize_dataset.tar.gz\n","tar -xf download/training_set.tar\n","```\n","\n","When we download the data, there are two important files:\n","\n","1) The file `training_set.tar` is a tar of a directory containing 17770 files, one per movie.  The first line of each file contains the movie id followed by a colon.  Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\n","\n","`CustomerID, Rating, Date`\n","- MovieIDs range from 1 to 17770 sequentially.\n","- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n","- Ratings are on a five star (integral) scale from 1 to 5.\n","- Dates have the format YYYY-MM-DD.\n","\n","2) Movie information in [`movie_titles.txt`](data/movie_titles.txt) is in the following format:\n","\n","`MovieID, YearOfRelease, Title`\n","\n","- MovieID do not correspond to actual Netflix movie ids or IMDB movie ids.\n","- YearOfRelease can range from 1890 to 2005 and may correspond to the release of corresponding DVD, not necessarily its theaterical release.\n","- Title in English is the Netflix movie.\n","\n","### Data prep\n","\n","The first step is to covert the data to the correct format for the autoencoder to read. This can take between 1 to 2 hours.  "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HetOcdTXfkgs","executionInfo":{"status":"ok","timestamp":1651240374140,"user_tz":240,"elapsed":168,"user":{"displayName":"Cibaca Pramod Khandelwal","userId":"16594370059923439368"}},"outputId":"a129c182-00b4-4122-a31d-4a0c4b29de15"},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:File `'./content/gdrive/MyDrive/dl-rl/recommendation-systems//DeepRecommender/data_utils/data_convert.py'` not found.\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1.51 ms, sys: 72 µs, total: 1.58 ms\n","Wall time: 1.44 ms\n"]}],"source":["%%time\n","%run ./content/gdrive/MyDrive/dl-rl/recommendation-systems//DeepRecommender/data_utils/data_convert.py $NF_PRIZE_DATASET $NF_DATA"]},{"cell_type":"markdown","metadata":{"id":"m8PFEHUhfkgs"},"source":["The script splitted the data into train, test and validation set, creating files with three columns: `CustomerID,MovieID,Rating`. The data is splitted over time generating 4 datasets: Netflix 3months, Netflix 6 months, Netflix 1 year and Netflix full. Here there is a table with some details of each dataset:\n","\n","| Dataset  | Netflix 3 months | Netflix 6 months | Netflix 1 year | Netflix full |\n","| -------- | ---------------- | ---------------- | ----------- |  ------------ |\n","| Ratings train | 13,675,402 | 29,179,009 | 41,451,832 | 98,074,901 |\n","| Users train | 311,315 |390,795  | 345,855 | 477,412 |\n","| Items train | 17,736 |17,757  | 16,907 | 17,768 |\n","| Time range train | 2005-09-01 to 2005-11-31 | 2005-06-01 to 2005-11-31 | 2004-06-01 to 2005-05-31 | 1999-12-01 to 2005-11-31\n","|  |  |  |   | |\n","| Ratings test | 2,082,559 | 2,175,535  | 3,888,684| 2,250,481 |\n","| Users test | 160,906 | 169,541  | 197,951| 173,482 |\n","| Items test | 17,261 | 17,290  | 16,506| 17,305 |\n","| Time range test | 2005-12-01 to 2005-12-31 | 2005-12-01 to 2005-12-31 | 2005-06-01 to 2005-06-31 | 2005-12-01 to 2005-12-31\n","\n","Let's take a look at some of the files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ao5FMfPNfkgs"},"outputs":[],"source":["nf_3m_valid = os.path.join(NF_DATA, 'N3M_VALID', 'n3m.valid.txt')\n","df = pd.read_csv(nf_3m_valid, names=['CustomerID','MovieID','Rating'], sep='\\t')\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQ-c2653fkgt","outputId":"4a8f55e4-a3e6-4064-fefb-aae55557e8a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1040820, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CustomerID</th>\n","      <th>MovieID</th>\n","      <th>Rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>159</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>4830</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1261</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>12058</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>13412</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   CustomerID  MovieID  Rating\n","0           0      159     4.0\n","1           0     4830     1.0\n","2           0     1261     3.0\n","3           0    12058     3.0\n","4           0    13412     2.0"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["nf_3m_test = os.path.join(NF_DATA, 'N3M_TEST', 'n3m.test.txt')\n","df2 = pd.read_csv(nf_3m_test, names=['CustomerID','MovieID','Rating'], sep='\\t')\n","print(df2.shape)\n","df2.head()"]},{"cell_type":"markdown","metadata":{"id":"w3JwP2a9fkgt"},"source":["## Deep Autoencoder for Collaborative Filtering\n","\n","Once we have the data, let's explain in some detail the model that we are going to use. The [model](https://arxiv.org/abs/1708.01715), developed by NVIDIA folks, is a Deep autoencoder with 6 layers with non-linear activation function SELU (scaled exponential linear units), dropout and iterative dense refeeding.\n","\n","An autoencoder is a network which implements two transformations: $encode(x): R^n \\Rightarrow R^d$ and $decoder(z): R^d \\Rightarrow R^n$. The “goal” of autoencoder is to obtain a $d$ dimensional representation of data such that an error measure between $x$ and $f(x) = decode(encode(x))$ is minimized. In the next figure, the autocoder architecture proposed in the [paper](https://arxiv.org/abs/1708.01715) is showed. Encoder has 2 layers $e_1$ and $e_2$ and decoder has 2 layers $d_1$ and $d_2$. Dropout may be applied to coding layer $z$. In the paper, the authors show experiments with different number of layers, from 2 to 12 (see Table 2 in the original paper).\n","\n","<p align=\"center\">\n","    <img src=\"./data/AutoEncoder.png\" width=350px/>\n","</p>\n","\n","During the forward pass the model takes a user representation by his vector of ratings from the training set $x \\in R^n$, where $n$ is number of items. Note that $x$ is very sparse, while the output of the decoder, $y=f(x) \\in R^n$ is dense and contains the rating predictions for all items in the corpus. The loss is the root mean squared error (RMSE).\n","\n","One of the key ideas of the paper is dense re-feeding. Let's consider an idealized scenario with a perfect $f$. Then $f(x)_i = x_i ,\\forall i : x_i \\ne 0$ and $f(x)_i$ accurately predicts all user's future ratings. This means that if a user rates a new item $k$ (thereby creating a new vector $x'$) then $f(x)_k = x'_k$ and $f(x) = f(x')$. Therefore, the authors refeed the input in the autoencoder to augment the dataset. The method consists of the following steps:\n","\n","1. Given a sparse $x$, compute the forward pass to get $f(x)$ and the loss.\n","\n","2. Backpropagate the loss and update the weights.\n","\n","3. Treat $f(x)$ as a new example and compute $f(f(x))$\n","\n","4. Compute a second backward pass.\n","\n","Steps 3 and 4 can be repeated several times.\n","\n","Finally, the authors explore different non-linear [activation functions](https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py). They found that on this task ELU, SELU and LRELU, which have non-zero negative parts, perform much better than SIGMOID, RELU, RELU6, and TANH.\n","\n","Now, let's compute the training. The model parameters can be found in [parameters.py](parameters.py)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2GKiYL4fkgt","outputId":"e5d97d62-f283-4dff-ea17-fcb5574377e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(aug_step=1, batch_size=128, constrained=False, drop_prob=0.8, gpu_ids='0', hidden_layers='512,512,1024', logdir='model_save', lr=0.005, noise_prob=0.0, non_linearity_type='selu', num_epochs=10, optimizer='momentum', path_to_eval_data='Netflix/N3M_VALID', path_to_train_data='Netflix/N3M_TRAIN', skip_last_layer_nl=False, weight_decay=0.0)\n","Loading training data from Netflix/N3M_TRAIN\n","Data loaded\n","Total items found: 311315\n","Vector dim: 17736\n","Loading eval data from Netflix/N3M_VALID\n","******************************\n","******************************\n","[17736, 512, 512, 1024]\n","Dropout drop probability: 0.8\n","Encoder pass:\n","torch.Size([512, 17736])\n","torch.Size([512])\n","torch.Size([512, 512])\n","torch.Size([512])\n","torch.Size([1024, 512])\n","torch.Size([1024])\n","Decoder pass:\n","torch.Size([512, 1024])\n","torch.Size([512])\n","torch.Size([512, 512])\n","torch.Size([512])\n","torch.Size([17736, 512])\n","torch.Size([17736])\n","******************************\n","******************************\n","Using GPUs: [0]\n","Doing epoch 0 of 10\n","Total epoch 0 finished in 69.39120125770569 seconds with TRAINING RMSE loss: 1.1183533288603893\n","Epoch 0 EVALUATION LOSS: 0.997187149064757\n","Doing epoch 1 of 10\n","Total epoch 1 finished in 69.05308318138123 seconds with TRAINING RMSE loss: 0.9789836858425376\n","Epoch 1 EVALUATION LOSS: 0.9830844731127444\n","Doing epoch 2 of 10\n","Total epoch 2 finished in 68.9950942993164 seconds with TRAINING RMSE loss: 0.9593065493420863\n","Epoch 2 EVALUATION LOSS: 0.98861261075587\n","Doing epoch 3 of 10\n","Total epoch 3 finished in 69.13668775558472 seconds with TRAINING RMSE loss: 0.9464339257342224\n","Epoch 3 EVALUATION LOSS: 0.9762008394628371\n","Doing epoch 4 of 10\n","Total epoch 4 finished in 69.09636783599854 seconds with TRAINING RMSE loss: 0.9355610656442712\n","Epoch 4 EVALUATION LOSS: 0.9808872814542353\n","Doing epoch 5 of 10\n","Total epoch 5 finished in 69.17095017433167 seconds with TRAINING RMSE loss: 0.9258845933913559\n","Epoch 5 EVALUATION LOSS: 0.9842890565637975\n","Doing epoch 6 of 10\n","Total epoch 6 finished in 69.08278322219849 seconds with TRAINING RMSE loss: 0.9622985019130311\n","Epoch 6 EVALUATION LOSS: 0.9838701840431763\n","Doing epoch 7 of 10\n","Total epoch 7 finished in 69.04743361473083 seconds with TRAINING RMSE loss: 0.9337027765558531\n","Epoch 7 EVALUATION LOSS: 0.984365107262594\n","Doing epoch 8 of 10\n","Total epoch 8 finished in 69.1117844581604 seconds with TRAINING RMSE loss: 0.9205498934527924\n","Epoch 8 EVALUATION LOSS: 0.9750949946703523\n","Doing epoch 9 of 10\n","Total epoch 9 finished in 69.13744783401489 seconds with TRAINING RMSE loss: 0.9099730840527471\n","Epoch 9 EVALUATION LOSS: 0.9742177051690417\n","Saving model to model_save/model.epoch_9\n","Routine finished. Process time 2747.513389825821 s\n"]}],"source":["%run ./DeepRecommender/run.py --gpu_ids $GPUS \\\n","    --path_to_train_data $TRAIN \\\n","    --path_to_eval_data $EVAL \\\n","    --hidden_layers $HIDDEN \\\n","    --non_linearity_type $ACTIVATION \\\n","    --batch_size $BATCH_SIZE \\\n","    --logdir $MODEL_OUTPUT_DIR \\\n","    --drop_prob $DROPOUT \\\n","    --optimizer $OPTIMIZER \\\n","    --lr $LR \\\n","    --weight_decay $WD \\\n","    --aug_step $AUG_STEP \\\n","    --num_epochs $EPOCHS "]},{"cell_type":"markdown","metadata":{"id":"A9ClLDYYfkgt"},"source":["## Evaluation\n","Now we are going to evaluate the model on the test set and compute the final loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5t1zqCtfkgt","outputId":"c08597fb-41ad-49df-f5a4-dc17461ee547"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(constrained=False, drop_prob=0.8, hidden_layers='512,512,1024', non_linearity_type='selu', path_to_eval_data='Netflix/N3M_TEST', path_to_train_data='Netflix/N3M_TRAIN', predictions_path='preds.txt', save_path='model_save/model.epoch_9', skip_last_layer_nl=False)\n","Loading training data\n","Data loaded\n","Total items found: 311315\n","Vector dim: 17736\n","Loading eval data\n","******************************\n","******************************\n","[17736, 512, 512, 1024]\n","Dropout drop probability: 0.8\n","Encoder pass:\n","torch.Size([512, 17736])\n","torch.Size([512])\n","torch.Size([512, 512])\n","torch.Size([512])\n","torch.Size([1024, 512])\n","torch.Size([1024])\n","Decoder pass:\n","torch.Size([512, 1024])\n","torch.Size([512])\n","torch.Size([512, 512])\n","torch.Size([512])\n","torch.Size([17736, 512])\n","torch.Size([17736])\n","******************************\n","******************************\n","Loading model from: model_save/model.epoch_9\n","Done: 0\n","Done: 10000\n","Done: 20000\n","Done: 30000\n","Done: 40000\n","Done: 50000\n","Done: 60000\n","Done: 70000\n","Done: 80000\n","Done: 90000\n","Done: 100000\n","Done: 110000\n","Done: 120000\n","Routine finished. Process time 228.1412889957428 s\n"]}],"source":["%run ./DeepRecommender/infer.py \\\n","--path_to_train_data $TRAIN \\\n","--path_to_eval_data $TEST \\\n","--hidden_layers $HIDDEN \\\n","--non_linearity_type $ACTIVATION \\\n","--save_path  $MODEL_PATH \\\n","--drop_prob $DROPOUT \\\n","--predictions_path $INFER_OUTPUT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPN90rj-fkgu","outputId":"35e3d4a3-3ab8-4da4-c73c-cc7d37454178"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(path_to_predictions='preds.txt', round=False)\n","####################\n","RMSE: 0.9746437597050387\n","####################\n"]}],"source":["%run ./DeepRecommender/compute_RMSE.py --path_to_predictions=$INFER_OUTPUT"]},{"cell_type":"markdown","metadata":{"id":"Elh1QBCcfkgx"},"source":["## Conclusion\n","In this notebook we showed how to create a recommendation system with a deep autoencoder. \n","\n","\n","Happy coding!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-Ph1jO3fkgx"},"outputs":[],"source":[" "]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Intro_Recommender.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}